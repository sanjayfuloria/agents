{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab2 - Multi-Model Comparison (Executable Version)\n",
    "\n",
    "This is an executable version of Lab2 that includes error handling and works with or without API keys.\n",
    "\n",
    "**What this lab demonstrates:**\n",
    "- Multi-Model Comparison pattern\n",
    "- Working with multiple AI provider APIs\n",
    "- Using AI judges to evaluate responses\n",
    "- Graceful error handling\n",
    "\n",
    "**Requirements:**\n",
    "- At least one API key in .env file (or use Ollama for free local models)\n",
    "- Internet connection for API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    anthropic_available = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Anthropic not installed (optional)\")\n",
    "    anthropic_available = False\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "print(\"âœ… Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API keys and show status\n",
    "def check_key(key_name, prefix_len=8):\n",
    "    key = os.getenv(key_name)\n",
    "    if key and key != f'your_{key_name.lower()}_here':\n",
    "        print(f\"âœ… {key_name} exists and begins {key[:prefix_len]}...\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {key_name} not set or using placeholder\")\n",
    "        return False\n",
    "\n",
    "print(\"ğŸ”‘ API Key Status:\")\n",
    "openai_ok = check_key('OPENAI_API_KEY')\n",
    "anthropic_ok = check_key('ANTHROPIC_API_KEY', 7)\n",
    "google_ok = check_key('GOOGLE_API_KEY', 2)\n",
    "deepseek_ok = check_key('DEEPSEEK_API_KEY', 3)\n",
    "groq_ok = check_key('GROQ_API_KEY', 4)\n",
    "\n",
    "if not any([openai_ok, anthropic_ok, google_ok, deepseek_ok, groq_ok]):\n",
    "    print(\"\\nâš ï¸ No API keys found. This will use fallback data or fail gracefully.\")\n",
    "    print(\"ğŸ’¡ For best results, add at least one API key to your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a challenging question\n",
    "if openai_ok:\n",
    "    try:\n",
    "        request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "        request += \"Answer only with the question, no explanation.\"\n",
    "        messages = [{\"role\": \"user\", \"content\": request}]\n",
    "        \n",
    "        openai_client = OpenAI()\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        question = response.choices[0].message.content\n",
    "        print(f\"âœ… Generated question: {question}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating question: {e}\")\n",
    "        question = \"Imagine you are tasked with explaining the concept of emergence to someone who has never encountered it before. How would you illustrate this concept using three different examples from completely different domains (biological, social, and technological), and what underlying principles connect these seemingly disparate phenomena?\"\n",
    "        print(f\"ğŸ“‹ Using fallback question: {question}\")\nelse:\n",
    "    question = \"Imagine you are tasked with explaining the concept of emergence to someone who has never encountered it before. How would you illustrate this concept using three different examples from completely different domains (biological, social, and technological), and what underlying principles connect these seemingly disparate phenomena?\"\n",
    "    print(f\"ğŸ“‹ Using fallback question (no OpenAI key): {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking lists\n",
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "print(f\"ğŸ¯ Testing {len([k for k in [openai_ok, anthropic_ok, google_ok, deepseek_ok, groq_ok] if k])} model(s) with the question...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI GPT-4o-mini\n",
    "if openai_ok:\n",
    "    try:\n",
    "        model_name = \"gpt-4o-mini\"\n",
    "        print(f\"ğŸ¤– Testing {model_name}...\")\n",
    "        \n",
    "        response = openai_client.chat.completions.create(model=model_name, messages=messages)\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"âœ… {model_name} responded successfully\")\n",
    "        print(f\"ğŸ“ Preview: {answer[:100]}...\")\n",
    "        competitors.append(model_name)\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {e}\")\nelse:\n",
    "    print(\"â­ï¸ Skipping OpenAI (no API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Anthropic Claude\n",
    "if anthropic_ok and anthropic_available:\n",
    "    try:\n",
    "        model_name = \"claude-3-5-sonnet-20241022\"\n",
    "        print(f\"ğŸ¤– Testing {model_name}...\")\n",
    "        \n",
    "        claude = Anthropic()\n",
    "        response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "        answer = response.content[0].text\n",
    "        \n",
    "        print(f\"âœ… {model_name} responded successfully\")\n",
    "        print(f\"ğŸ“ Preview: {answer[:100]}...\")\n",
    "        competitors.append(model_name)\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {e}\")\nelse:\n",
    "    print(\"â­ï¸ Skipping Anthropic (no API key or not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google Gemini\n",
    "if google_ok:\n",
    "    try:\n",
    "        model_name = \"gemini-2.0-flash\"\n",
    "        print(f\"ğŸ¤– Testing {model_name}...\")\n",
    "        \n",
    "        google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "        response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"âœ… {model_name} responded successfully\")\n",
    "        print(f\"ğŸ“ Preview: {answer[:100]}...\")\n",
    "        competitors.append(model_name)\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {e}\")\nelse:\n",
    "    print(\"â­ï¸ Skipping Google Gemini (no API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DeepSeek\n",
    "if deepseek_ok:\n",
    "    try:\n",
    "        model_name = \"deepseek-chat\"\n",
    "        print(f\"ğŸ¤– Testing {model_name}...\")\n",
    "        \n",
    "        deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "        deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "        response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"âœ… {model_name} responded successfully\")\n",
    "        print(f\"ğŸ“ Preview: {answer[:100]}...\")\n",
    "        competitors.append(model_name)\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {e}\")\nelse:\n",
    "    print(\"â­ï¸ Skipping DeepSeek (no API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Groq Llama\n",
    "if groq_ok:\n",
    "    try:\n",
    "        model_name = \"llama-3.3-70b-versatile\"\n",
    "        print(f\"ğŸ¤– Testing {model_name}...\")\n",
    "        \n",
    "        groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "        groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"âœ… {model_name} responded successfully\")\n",
    "        print(f\"ğŸ“ Preview: {answer[:100]}...\")\n",
    "        competitors.append(model_name)\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {e}\")\nelse:\n",
    "    print(\"â­ï¸ Skipping Groq (no API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ollama (local model)\n",
    "try:\n",
    "    model_name = \"llama3.2 (Ollama)\"\n",
    "    print(f\"ğŸ¤– Testing {model_name}...\")\n",
    "    \n",
    "    ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    print(f\"âœ… {model_name} responded successfully\")\n",
    "    print(f\"ğŸ“ Preview: {answer[:100]}...\")\n",
    "    competitors.append(model_name)\n",
    "    answers.append(answer)\n",
    "except Exception as e:\n",
    "    print(f\"â­ï¸ Skipping Ollama: {e}\")\n",
    "    print(\"ğŸ’¡ To use Ollama: 1) Install from ollama.com 2) Run 'ollama serve' 3) Run 'ollama pull llama3.2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current results\n",
    "print(f\"\\nğŸ“Š Results so far: {len(competitors)} models responded successfully\")\n",
    "for i, competitor in enumerate(competitors):\n",
    "    print(f\"   {i+1}. {competitor}\")\n",
    "\n",
    "if len(competitors) == 0:\n",
    "    print(\"\\nâŒ No models responded. Check your .env file or install Ollama for local testing.\")\n",
    "elif len(competitors) == 1:\n",
    "    print(\"\\nâš ï¸ Only one model responded. Need at least 2 for judging.\")\nelse:\n",
    "    print(\"\\nâœ… Multiple models responded. Ready for judging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full responses\n",
    "if len(competitors) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" FULL RESPONSES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for competitor, answer in zip(competitors, answers):\n",
    "        print(f\"\\nğŸ¤– {competitor}:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(answer)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge the responses (if we have multiple models and OpenAI)\n",
    "if len(competitors) >= 2 and openai_ok:\n",
    "    try:\n",
    "        print(\"\\nâš–ï¸ JUDGING PHASE\")\n",
    "        print(\"=================\")\n",
    "        \n",
    "        # Prepare responses for judging\n",
    "        together = \"\"\n",
    "        for index, answer in enumerate(answers):\n",
    "            together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "            together += answer + \"\\n\\n\"\n",
    "        \n",
    "        judge_prompt = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n",
    "        \n",
    "        judge_messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "        \n",
    "        print(\"ğŸ¤– Asking o3-mini to judge the responses...\")\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"o3-mini\",\n",
    "            messages=judge_messages,\n",
    "        )\n",
    "        results = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"ğŸ“Š Raw judgment: {results}\")\n",
    "        \n",
    "        # Parse results\n",
    "        results_dict = json.loads(results)\n",
    "        ranks = results_dict[\"results\"]\n",
    "        \n",
    "        print(\"\\nğŸ† FINAL RANKINGS:\")\n",
    "        for index, result in enumerate(ranks):\n",
    "            competitor = competitors[int(result)-1]\n",
    "            print(f\"   Rank {index+1}: {competitor}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during judging: {e}\")\n",
    "        print(\"ğŸ’¡ Judgment failed, but you can still compare the responses manually above!\")\n",
    "        \n",
    "elif len(competitors) < 2:\n",
    "    print(\"\\nâš ï¸ Need at least 2 model responses for judging.\")\n",
    "elif not openai_ok:\n",
    "    print(\"\\nâš ï¸ OpenAI API key required for judging. You can still compare responses manually above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Lab2 Complete!\n",
    "\n",
    "**What you just experienced:**\n",
    "- **Multi-Model Comparison Pattern**: Multiple AI models tackling the same challenge\n",
    "- **API Integration**: Working with different AI provider APIs\n",
    "- **Response Evaluation**: Using an AI judge to rank outputs\n",
    "- **Error Handling**: Graceful degradation when services aren't available\n",
    "\n",
    "**Commercial Applications:**\n",
    "- Quality assurance in AI systems\n",
    "- Model selection for specific tasks\n",
    "- Ensemble methods for better accuracy\n",
    "- A/B testing AI models in production\n",
    "\n",
    "**Next Steps:**\n",
    "1. Try adding more API keys to test additional models\n",
    "2. Experiment with different questions\n",
    "3. Modify the judging criteria\n",
    "4. Use this pattern in your own projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}